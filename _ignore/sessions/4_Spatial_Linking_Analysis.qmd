---
title: "Spatial Linking & Analysis"
author: "Stefan Jünger & Dennis Abel"
date: April 09, 2025
# institute: GESIS Workshop
execute:
  echo: true
format:
  revealjs:
    embed-resources: true
    theme: [simple, tweaks.css]
    smaller: true
    scrollable: true
    slide-number: "c/t"
    navigation-mode: vertical
    # footer: "GESIS Workshop: Introduction to Geospatial Techniques for Social Scientists in R"
    logo: ../img/GESIS-Logo_2024.svg.png
    toc: false
editor_options: 
  chunk_output_type: console
---

```{r}
#| include: false
library(dplyr)
library(sf)
library(terra)
library(tmap)
```

## Now

```{r}
#| echo: false
source("course_content.R") 

course_content |> 
  kableExtra::row_spec(7, background = "yellow")
```

## Mini wrap-up

Thus far, we have learned about

- different data formats
- how to load them
- first steps in interacting with them
- creating maps with `tmap`

But the real magic in geospatial data lies in their flexibility.

## Our plan for this afternoon

In this session, you will learn

- how to wrangle the different geospatial data formats even further
- how to converse from one format into the other

Because we want to

- link/join different datasets
- do some spatial analysis

## 1. Advanced Importing {.center style="text-align: center;"}

## Importing of non-spatial data

Say, the data we want to use are not available as shapefile. Point coordinates are often stored in table formats like `.csv` -- as is the location of charging stations for electric cars data in our `./data` folder. 

```{r}
#| eval: false
echarging_df <- 
  readr::read_delim("./data/charging_points_ger.csv", delim = ";")

head(echarging_df)
```

```{r}
#| echo: false
echarging_df <- 
  readr::read_delim("../../data/charging_points_ger.csv", delim = ";")

head(echarging_df)
```

## From data table to geospatial data

We see that besides our attributes (e.g., operator, power,...), the table contains the two variables "longitude" (X) and "latitude" (Y), our point coordinates. When using the command `sf::st_as_sf()`, it is easy to transform the table into a point layer.

```{r}
# transform to spatial data frame
echarging_sf <- 
  sf::st_as_sf(
    echarging_df |>  
      # there were some missings in my data which is not allowed 
      dplyr::filter(!is.na(longitude) & !is.na(latitude)),    
    coords = c("longitude", "latitude")
  )

# inspect data
class(echarging_sf)
```

## Final check

```{r}
plot(echarging_sf)
```

## Check the CRS!

Make sure to use the option `crs = [EPSG_ID]`. If not used, your CRS will not be defined, and you can't perform further commands depending on the CRS. Here, I tried [EPSG IO](https://epsg.io) or [http://projfinder.com/](http://projfinder.com/) to find out.

```{r}
echarging_sf <- 
  sf::st_as_sf(
    echarging_df |>  
      # there were some missings in my data which is not allowed  
      dplyr::filter(!is.na(longitude) & !is.na(latitude)),    
    coords = c("longitude", "latitude"),
    crs = 4326
  )

plot(echarging_sf)
```

## ... and the other way round

Do you want to go back to handling a simple data frame? You can quickly achieve this by dropping the geometry column.

```{r}
# remove geometry column
sf::st_drop_geometry(echarging_sf) |>  
  head(2)
```

## APIs

Geospatial data tend to be quite big
- pressure to distribute data efficiently

Data dumps (on the internet) may not be helpful 
- when resources are low 
- time's a factor
- the data have a large geographic extent

Often a *Programming Application Interfaces* (API) is used

## Data providers offering geospatial data APIs

- [OpenStreetMap](https://wiki.openstreetmap.org/wiki/API)
- [Google](https://developers.google.com/maps/documentation/geolocation/overview)
- [Bing](https://docs.microsoft.com/en-us/bingmaps/rest-services/locations/)
- [Copernicus Climate Data Store](https://cds.climate.copernicus.eu/)
- ...
- [Cologne's Open Data Portal](https://www.offenedaten-koeln.de/dataset/taxonomy/term/44/field_tags/Geo-44)
- Specialized `R` packages, such as the [`wiesbaden` package](https://cran.r-project.org/web/packages/wiesbaden/index.html) or the [`tidycensus` package](https://cran.r-project.org/web/packages/tidycensus/index.html)

## Example: access to public transport

Say, we're interested in the accessibility of public transport in Cologne
- bus, tram, etc.
- all platforms and vehicles should be wheel-chair accessible

**We can gather this information using OpenStreetMap!**

## Accessing OSM data: the Overpass API

> The Overpass API (formerly known as OSM Server Side Scripting, or OSM3S before 2011) is a read-only API that serves up custom selected parts of the OSM map data. It acts as a database over the web: the client sends a query to the API and returns the data set that corresponds to the query.

<small>Source: https://wiki.openstreetmap.org/wiki/Overpass_API</small>

---

## Starting with a geographic area to query

Many geospatial API requests start with a bounding box or another geographical extent to define which area should be accessed.

```{r}
cologne_pt_stops <-
  osmdata::getbb(
    "Köln"
  )

cologne_pt_stops
```

## Defining some technical details

The Overpass API also requires a timeout parameter that repeats the request for a while if anything fails.

```{r}
cologne_pt_stops <-
  cologne_pt_stops |> 
  osmdata::opq(timeout = 25*100)

cologne_pt_stops
```

## Turning to the content

The content we aim to request is defined with key/value pairs. It's best to learn them by looking them up in the [official documentation](https://wiki.openstreetmap.org/wiki/Map_features).

```{r}
cologne_pt_stops <-
  cologne_pt_stops |>    
  osmdata::add_osm_feature(key = "public_transport", value = "stop_position")

cologne_pt_stops
```


## Conduct actual request/download

The data is finally downloaded in the `osmdata` package, e.g., using the `osmdata::osmdata_sf()` function.

```{r}
cologne_pt_stops <-
  cologne_pt_stops |>   
  osmdata::osmdata_sf()

cologne_pt_stops
```

## Filter and transform

The data comprises a list that can be accessed as any list in `R`. Here, we extract the points and wrangle them (spatially).

```{r}
cologne_pt_stops <-
  cologne_pt_stops$osm_points |> 
  tibble::as_tibble() |> 
  sf::st_as_sf() |> 
  dplyr::filter(wheelchair == "yes")

cologne_pt_stops
```

## The data indeed are mappable

```{r}
tm_shape(cologne_pt_stops) +
  tm_dots()
```

## Example: GeoJSON files

JSON files are pretty popular
- standardized and well-structured
- similar to XML-files, but human readability is a bit better
- also easy to parse for editors and browser

There's also a JSON flavor for geospatial data...

## GeoJSON snippet

```
{
"type": "FeatureCollection",
"features": [
{
"type": "Feature",
"id": 12,
"geometry": {
"type": "Polygon",
"coordinates": [
[
[
6.957362270020273,
50.94308762750329
]   
...
```

<small>Source: https://www.offenedaten-koeln.de/</small>



## An application from Cologne’s Open Data portal

```{r}
park_and_ride <-
  glue::glue(
    "https://geoportal.stadt-koeln.de/arcgis/rest/services/verkehr/",
    "verkehrskalender/MapServer/8/query?where=objectid+is+not+null&text=&",
    "objectIds=&time=&geometry=&geometryType=esriGeometryEnvelope&inSR=&",
    "spatialRel=esriSpatialRelIntersects&distance=&units=esriSRUnit_Foot&",
    "relationParam=&outFields=*&returnGeometry=true&returnTrueCurves=false&",
    "maxAllowableOffset=&geometryPrecision=&outSR=4326&havingClause=&",
    "returnIdsOnly=false&returnCountOnly=false&orderByFields=&",
    "groupByFieldsForStatistics=&outStatistics=&returnZ=false&returnM=false&",
    "gdbVersion=&historicMoment=&returnDistinctValues=false&resultOffset=&",
    "resultRecordCount=&returnExtentOnly=false&datumTransformation=&",
    "parameterValues=&rangeValues=&quantizationParameters=&featureEncoding=",
    "esriDefault&f=pjson"
  ) |> 
  sf::st_read(as_tibble = TRUE)
```

<small>Source: https://www.offenedaten-koeln.de/dataset/park-and-ride-anlagen-koeln</small>

## Park & ride parking spaces

With just two 'simple' commands, we can retrieve geospatial data about Cologne's Park & Ride parking spaces in `R`. Not too bad, right?

```{r}
tm_shape(park_and_ride) +
  tm_dots()
```

## Raster data access

It is not only vector data that can be processed through these mechanisms.

The idea is the same for raster data
- accessing the information through URLs
- just the downloaded data formats differ

## Example: downloading German weather data

The German Weather Service provides excellent weather data in its Climate Data Center (CDC): https://opendata.dwd.de/climate_environment/CDC/

Let's download some summer temperature data using a custom function.

```{r}
download_dwd_data <- function(url, path) {
  
  download.file(url, dest = paste0(path, "/", "lyr.1.asc.gz"))
  
  R.utils::gunzip(
    paste0(path, "/", "lyr.1.asc.gz"), 
    overwrite = TRUE,
    remove = TRUE
  )
  
  raster_file <-
    terra::rast(paste0(path, "/", "lyr.1.asc"))
  
  unlink(paste0(path, "/", "lyr.1.asc.gz"))
  
  raster_file
}
```

## Voilà

```{r}
#| eval: false
paste0(
  "https://opendata.dwd.de/climate_environment/CDC/grids_germany/seasonal/",
  "air_temperature_max/14_JJA/grids_germany_seasonal_air_temp_max_202314.",
  "asc.gz") |> 
  download_dwd_data(path = "./data/") |> 
  terra::plot()
```

```{r}
#| echo: false
paste0(
  "https://opendata.dwd.de/climate_environment/CDC/grids_germany/seasonal/",
  "air_temperature_max/14_JJA/grids_germany_seasonal_air_temp_max_202314.",
  "asc.gz") |> 
  download_dwd_data(path = "../../data/") |> 
  terra::plot()
```


```{r}
#| include: false
unlink("./data/lyr.1.asc")
```

## 2. Linking {.center style="text-align: center;"}

## 'Spreadsheet join'

While a lot of our previous data were points, we only had a column for the German federal states as administrative information so far. We're now moving "a layer down" and looking at Germany on a more fine-grained spatial level: the district. We just repeat what you already have done in the exercise `1_3_1_Basic Maps`: joining data just like simple spreadsheets.

```{r}
#| eval: false
# load district shapefile
german_districts <- sf::read_sf("./data/VG250_KRS.shp")

# load district attributes
attributes_districts <- readr::read_csv2("./data/attributes_districts.csv") 

# join data
german_districts_enhanced <- 
  german_districts |>  
  dplyr::left_join(attributes_districts, by = "AGS")
```

```{r}
#| echo: false
# load district shapefile
german_districts <- sf::read_sf("../../data/VG250_KRS.shp")

# load district attributes
attributes_districts <- readr::read_csv2("../../data/attributes_districts.csv") 

# join data
german_districts_enhanced <- 
  german_districts |>  
  dplyr::left_join(attributes_districts, by = "AGS")
```

## Spatial join

But what can we do, if we do not have matching identifier. For example, in the German districts data and the e-charger data, there are no matching administrative identifier.

```{r}
german_districts_enhanced
```

```{r}
echarging_sf
```

We conduct a spatial join!

## Adding district information to the point layer

```{r}
echarging_sf_joined <-
  echarging_sf |> 
  sf::st_transform(crs = sf::st_crs(german_districts)) |> 
  sf::st_join(german_districts |> dplyr::select(AGS))

plot(echarging_sf_joined["AGS"])
```

## Spatial join even easier

Say, we want to count the number of charging stations in each German district.

```{r}
# adjust crs first
echarging_sf <-
  sf::st_transform(echarging_sf, crs = sf::st_crs(german_districts))

# count all chargers in district
german_districts_enhanced$charger_in_districts <-
  lengths(sf::st_intersects(german_districts_enhanced, echarging_sf))

german_districts_enhanced$charger_in_districts
```

## Charger count per district

```{r}
tm_shape(german_districts_enhanced) +
  tm_polygons(
    fill = "charger_in_districts",
    fill.legend = tm_legend(title = "Charger Count (Quantiles)"),
    fill.scale = tm_scale_intervals(style = "quantile", values = "viridis"),
    col = "lightgrey"
  )
```

## 3. Subsetting {.center style="text-align: center;"}

## Subsetting vector data

One might be interested in only one specific area of Germany, like Cologne. To subset a `sf` object,  you can often use your usual data wrangling workflow. In this case, I know the AGS ID, and that is the only row I want to keep.

```{r}
# subsetting
cologne <-
  german_districts_enhanced |> 
  dplyr::filter(AGS == "05315") |>  
  dplyr::select(AGS) 

plot(cologne)
```

## Using `sf` for subsetting

If you have no information about *ids* but only about the geolocation, you can use `sf::st_touches()` (or `sf::st_touches()`, `sf::st_within()`, `sf::st_intersect()`, `sf::st_crosses()`, ...) to identify, for example, all districts which share a border with Cologne.

```{r}
cologne_surrounding <-
  german_districts_enhanced |> 
  dplyr::select(AGS) |>  
  # length of mutual border > 0
  dplyr::filter(lengths(sf::st_touches(german_districts_enhanced, cologne)) > 0) 

plot(cologne_surrounding)
```

## Cropping data

We can use our raster data to subset our e-charging data. But first, we have to adjust the CRS again (use `sf::st_crs(echarging_sf)` to look up EPSG code):

```{r}
#| eval: false
inhabitants_cologne <- terra::rast("./data/inhabitants_cologne.tif")

inhabitants_cologne <- terra::project(inhabitants_cologne, "EPSG:25832")
```

```{r}
#| echo: false
inhabitants_cologne <- terra::rast("../../data/inhabitants_cologne.tif")

inhabitants_cologne <- terra::project(inhabitants_cologne, "EPSG:25832")
```

Now we can crop the data:

```{r}
echarging_sf_cologne <-
  echarging_sf |> 
  sf::st_crop(inhabitants_cologne)
```

## E-charging stations in Cologne

```{r}
tm_shape(echarging_sf_cologne) +
  tm_dots()
```

## 'Subsetting' Raster Layers

As you know, we can subset vector data by simply filtering for specific attribute values. For example, to subset Cologne's districts only by the one of Deutz, we can use the `dplyr` for `sf` data:

```{r}
#| eval: false
deutz <-
  sf::st_read("./data/cologne.shp") |> 
  dplyr::filter(NAME == "Deutz") |> 
  sf::st_transform(25832)

tm_shape(deutz) +
  tm_borders()
```

```{r}
#| echo: false
deutz <-
  sf::st_read("../../data/cologne.shp") |> 
  dplyr::filter(NAME == "Deutz") |> 
  sf::st_transform(25832)

tm_shape(deutz) +
  tm_borders()
```


## Cropping raster data

Cropping is then a method of cutting out a specific `slice` of a raster layer based on an input dataset or geospatial extent, such as a bounding box.

```{r}
cropped_inhabitants_cologne <-
  terra::crop(inhabitants_cologne, deutz)

tm_shape(inhabitants_cologne) +
  tm_raster()
```

## Masking

Masking is similar to cropping, yet values outside the extent are set to missing values (`NA`).

```{r}
masked_inhabitants_cologne <-
  raster::mask(inhabitants_cologne, terra::vect(deutz))

tm_shape(masked_inhabitants_cologne) +
  tm_raster()
```

## Combining Cropping and Masking

```{r crop-mask-raster}
cropped_masked_inhabitants_cologne <-
  terra::crop(inhabitants_cologne, deutz) |> 
  terra::mask(terra::vect(deutz))

tm_shape(cropped_masked_inhabitants_cologne) +
  tm_raster()
```

## 4. Extraction & Aggregation {.center style="text-align: center;"}

In case, we only want to add one attribute from a vector dataset `Y` to another vector dataset `X` we can conduct a spatial join using `sf::st_join()` as before. There is nothing new to tell. In the raster data world, these kind of operations are called raster extractions.

## Extracting information from raster data

Raster data are helpful when we aim to

- apply calculations that are the same for all geometries in the dataset
- **extract information from raster fast and efficient**

Do you remember these data operations?

```{r}
#| eval: false
immigrants_cologne <-  terra::rast("./data/immigrants_cologne.tif")

inhabitants_cologne <- terra::rast("./data/inhabitants_cologne.tif")

immigrant_rate <-
  immigrants_cologne * 100 / 
  inhabitants_cologne
```

```{r}
#| echo: false
immigrants_cologne <-  terra::rast("../../data/immigrants_cologne.tif")

inhabitants_cologne <- terra::rast("../../data/inhabitants_cologne.tif")

immigrant_rate <-
  immigrants_cologne * 100 / 
  inhabitants_cologne
```

## Raster extraction

To extract the raster values at a specific point by location, we use the following:

```{r raster-extraction}
terra::extract(immigrant_rate, echarging_sf_cologne, ID = FALSE)
```

## Add results to existing dataset

This information can be added to an existing dataset (our points in this example):

```{r add-results}
echarging_sf_cologne$immigrant_rate_value <-
  terra::extract(immigrant_rate, echarging_sf_cologne, ID = FALSE) |> 
  unlist()

echarging_sf_cologne
```

## More elaborated: spatial buffers

Sometimes, extracting information 1:1 is not enough

- too narrow
- missing information about the surroundings of a point

```{r}
tm_shape(immigrant_rate) +
  tm_raster() +
  tm_shape(
    sf::st_buffer(echarging_sf_cologne, 500) 
  ) +
  tm_dots(size = .1) +
  tm_borders()
```

## Buffer extraction

We can use spatial buffers of different sizes to extract information on surroundings:

:::: columns
::: {.column width="50%"}
```{r}
terra::extract(
  immigrant_rate, 
  sf::st_buffer(echarging_sf_cologne, 500),
  fun = mean,
  na.rm = TRUE,
  ID = FALSE,
  raw = TRUE
)
```
:::

::: {.column width="50%"}
```{r}
terra::extract(
  immigrant_rate, 
  sf::st_buffer(echarging_sf_cologne, 1000),
  fun = mean,
  na.rm = TRUE,
  ID = FALSE,
  raw = TRUE
)
```
:::
::::

## Raster aggregation

We can use the same procedure to all aggregate a raster dataset to a vector polygon dataset. That's a very common use case. Let's reload our Cologne shapefile.

```{r}
#| eval: false
#| layout-ncol: 2
cologne <- 
  sf::st_read("./data/cologne.shp") |> 
  sf::st_transform(25832)

plot(cologne["NAME"])
```

```{r}
#| echo: false
#| layout-ncol: 2
cologne <- 
  sf::st_read("../../data/cologne.shp") |> 
  sf::st_transform(25832)

plot(cologne["NAME"])
plot(immigrant_rate)
```

## Add the aggregated data

```{r}
cologne$immigrant_rate <-
  terra::extract(
    immigrant_rate, 
    cologne, 
    fun = mean, 
    na.rm = TRUE, 
    ID = FALSE
  ) |> 
  unlist()

plot(cologne["immigrant_rate"])
```

## 5. Conversion & Analysis {.center style="text-align: center;"}

## Raster to points

```{r}
raster_now_points <-
  immigrant_rate |> 
  terra::as.points()

plot(raster_now_points)
```

## Points to raster

```{r}
raster_target_layer <- 
  terra::ext(raster_now_points) |> 
  terra::rast(res = 100)

points_now_raster <- 
  raster_now_points |> 
  terra::rasterize(
    raster_target_layer, 
    field = "immigrants_cologne", 
    fun = "mean",
    background = 0
    )

plot(points_now_raster)
```

## Raster to polygons

```{r}
polygon_raster <-
  immigrant_rate |>  
  terra::as.polygons() |> 
  sf::st_as_sf()

plot(polygon_raster)
```

## Analysis: creating a quick 'heatmap'

OpenStreetMap points data are nice for analyzing urban infrastructure. Let's draw a quick 'heatmap' using kernel densities.

```{r}
echarging_sf_cologne_raster <-
  terra::rast(echarging_sf_cologne, res = 1000)

echarging_sf_cologne_densities <- 
  echarging_sf_cologne |> 
  terra::rasterize(echarging_sf_cologne_raster, fun = length, background = 0)

plot(echarging_sf_cologne_densities)
```

## Focal statistics / spatial filter

Focal statistics are another method of including information near a point in space. However, it's applied to the whole dataset and is independent of arbitrary points we project onto a map.

- relates focal cells to surrounding cells
- vastly used in image processing
- but also applicable in social science research, as we will see

## Analysis: edge detection

:::: columns
::: {.column width="50%"}
![](../img/Bikesgray.jpg){fig-align="center" width="75%"}
:::
![](../img/Bikesgraysobel.jpg){fig-align="center" width="75%"}
::: {.column width="50%"}

:::
::::

<small>Source: https://en.wikipedia.org/wiki/Sobel_operator</small>

---

## Edges of immigrant rates

![](../img/legewie_schaeffer_2016.png){.r-stretch fig-align="center"}



## We can do that as well using a sobel filter

$$r_x = \begin{bmatrix}1 & 0 & -1 \\2 & 0 & -2 \\1 & 0 & -1\end{bmatrix} \times raster\_file \\r_y = \begin{bmatrix}1 & 2 & 1 \\0 & 0 & 0 \\-1 & -2 & -1\end{bmatrix}\times raster\_file \\r_{xy} = \sqrt{r_{x}^2 + r_{y}^2}$$

---

## Implementation in R

From the [official documentation](http://search.r-project.org/R/library/terra/html/focal.html):

```{r}
sobel <- function(r) {
  fy <- matrix(c(1, 0, -1, 2, 0, -2, 1, 0, -1), nrow = 3)
  fx <- matrix(c(-1, -2, -1, 0, 0, 0, 1, 2, 1) , nrow = 3)
  rx <- terra::focal(r, fx)
  ry <- terra::focal(r, fy)
  sqrt(rx^2 + ry^2)
}
```

## Data preparation and application of filter

```{r data-prep-filter}
old_extent <- terra::ext(immigrant_rate) 
new_extent <- old_extent + c(10000, -10000, 10000, -10000)

smaller_immigrant_rate <-
  immigrant_rate |> 
  terra::crop(new_extent)

smaller_immigrant_rate[smaller_immigrant_rate < 10] <- NA

immigrant_edges <- sobel(smaller_immigrant_rate)
```

---

## Comparison

```{r}
#| echo: true
#| layout-ncol: 2
plot(smaller_immigrant_rate)
plot(immigrant_edges)
```






